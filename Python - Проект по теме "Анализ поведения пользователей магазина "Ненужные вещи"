Проект по теме "Анализ поведения пользователей магазина << Ненужные вещи>>"


Описание проекта:
    Наши пользователи совершают много действий в приложении, и мы уверены, что в этих
    данных есть инсайты, которые позволят нам стать лучшим приложением для продажи 
    ненужных вещей.
    «Ненужные вещи» — ваши ненужные вещи нужны кому-то другому!

Задача:
  1 Проанализируйте связь целевого события — просмотра контактов — и других
  действий пользователей.

  2 Оцените, какие действия чаще совершают те пользователи, которые
  просматривают контакты.
  Проведите исследовательский анализ данных
  Проанализируйте влияние событий на совершение целевого события
  Этапы:


  Проведите исследовательский анализ данных

  Проанализируйте влияние событий на совершение целевого события

  Проверить гипотезы:
    1 Одни пользователи совершают действия tips_show и tips_click , другие —
    только tips_show . Проверьте гипотезу: конверсия в просмотры контактов
    различается у этих двух групп.

    2 Сформулируйте собственную статистическую гипотезу. Дополните её
    нулевой и альтернативной гипотезами. Проверьте гипотезу с помощью
    статистического теста.

import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
import numpy as np
import math as mth
import scipy.stats as stats
from scipy import stats as st
import plotly.express as px
from plotly import graph_objects as go
import warnings
warnings.filterwarnings('ignore')


1 Загрузка данных *(загрузка необходимых данных , первичный осмотр)

data_df = pd.read_csv('https://code.s3.yandex.net/datasets/mobile_dataset.csv')
source_df = pd.read_csv('https://code.s3.yandex.net/datasets/mobile_sources.csv')
​
display(data_df.head(5))
display(source_df.head(5))

display(data_df.info())
display(source_df.info())

      Два датафрейма:
      первый состоит из 3 колонок , 74197 строк. 
      второй из 2 колонок , 4293 строк.
      Видим что в большинстве типы данных приемлимые(object) , кроме колонки event.time , нужно  поменять на datetime, 
      так же стоит убрать милисекунды.
      Нужно заменить в названиях точки (event.time) на нижние пробелы (event_time) для всех столбцов data_df.
      В source_df поменять название колонки userId на user_id.


2  Предобработка данных *(посмотрим типы данных , изменим , если нелобходимо
                         проверим пропуски и обработаем если необходимо
                         проверим дубликаты)
 
  Поменяем тип данных и названия

  уберём милисекунды из event.time

data_df['event.time'] = pd.to_datetime(data_df['event.time'])
data_df['event.time'] = data_df['event.time'].dt.floor('s')
data_df.info()

data_df = data_df.rename(columns={"event.name" : "event_name", "event.time" : "event_time" , "user.id" : "user_id"})
source_df = source_df.rename(columns={"userId" : "user_id"})
display(data_df.head(5))
display(source_df.head(5))

  Все что нужно поменяли , теперь перейдём к дубликатам и пропускам

print('Количество дубликатов в таблице data_df:', data_df.duplicated().sum())

  Удалим дубликаты , их не очень много , почти не повлияет на общую выборку

data_df = data_df.drop_duplicates().reset_index(drop=True)

print(data_df.isna().sum())
print(source_df.isna().sum())
  
  пропусков не наблюдается
  ещё стоит посмотреть уникальные значения колонок с названиями действий пользователей и источника установки приложения

print(data_df['event_name'].unique())
print(source_df['source'].unique())

print('Количество дубликатов в общей таблице data:', data.duplicated().sum())
print('Количество пропущенных значений в общей таблице data:', data.isnull().sum())

  Предобработали данные и создали один общий датафрейм data для работы с ним
  добавим туда колонки с часом , неделей , месяцем , годом и просто датой без времени

data.head(5)

  Создадим резервные датафреймы

data1=data
data2=data
data3=data

display(data.query('event_name == "map"')['user_id'].nunique())
display(data2.query('event_name == "map"')['user_id'].nunique())
display(data3.query('event_name == "map"')['user_id'].nunique())

3  Исследовательский анализ данных *(необходимо найти повторяющиеся сценарии дейтсвий пользователей , 
                                    сгруппируем по сессиям,
                                    избавимся от повторяющихся событий в рамках сессии и 
                                    определим общую популярность сценариев,
                                    рассмотрим связь целевого события — просмотра контактов — и других действий пользователей,
                                    посмотрим какой самый частый сценарий у людей которые просматривают контакты,
                                    построим воронку сценариев по уникальным пользователям ,
                                    посмотрим конверсию переходов)
  
    Рассмотрим данные и немного исследуем их

3.1  Посчитаем долю из каждого источника установки
global_sc = data.groupby('source').agg({'user_id' : 'nunique'})

global_sc = global_sc.reset_index()
global_sc

global_sc['proportion'] = global_sc['user_id'] / len(data['user_id'].unique()) *100
global_sc

global_sc = global_sc.round(1)
global_sc

plt.figure(figsize=(10,5))
ax=sns.barplot(x=global_sc['source'],y=global_sc['proportion'], data = global_sc, color='steelblue')
​
#добавим числа для понимания
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.1f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha = 'center', va = 'center',
                   xytext = (0, 9),
                   textcoords = 'offset points')
​
​
ax.set_xlabel('Источник установки приложения')
ax.set_ylabel('Процент пользователей')
plt.title('Источники установки приложения')
​
plt.show()
    
    yandex на 1 месте среди всех источников скачивания приложений (1934 - 45.1%)
    other(другие) занимает второе место (1230 - 28.7%)
    далее идёт google (1129 - 26.3%)

3.2  посмотрим частоту действий пользователей
    
    для начала нужно привести в порядок колонку event_name
    нужно совместить contacts_show и show_contacts. Так же поступим с всеми поисковыми search_N(1-7)


def event_combination(i):
    
    if i == 'show_contacts':
        
        new_name = 'contacts_show'
        return new_name
    
    elif 'search' in i:
        
        new_name = 'search'
        return new_name
    
    return i
data['event_name'] = data['event_name'].apply(event_combination)
freq_events = data['event_name'].value_counts().to_frame()
freq_events

data['event_name'] = data['event_name'].apply(event_combination)
freq_events = data['event_name'].value_counts().to_frame()
freq_events

freq_events.plot(kind='bar', figsize=(10,5))

plt.title('Частота совершения действий', fontsize=15)

plt.gca().set(xlabel='типы событий', ylabel='количество событий')
plt.xticks(rotation=20)

plt.show()

    на первом месте находится tips_show - увидел рекомендованные объявлений , почти 40000 , оно учитывается автоматически ,
    не зависит от действий пользователя
    далее идуёт photos_show - просмотр фото в объявлении
    на 3 месте поиск по сайту
    4 место - advert_open - открыл карточки объявлений
    5 - map - открытие карты объявлений
    favorites_add , tips_click , contacts_call занимают 3 последних места - избранное , клик по рекомендации и звонок
    tips_show - сильно больше любых других действий, вызвано это тем что люди постоянно смотрят рекомендованные объявления в приложении с продажей для покупки

3.3  Retention rate
  посмотрим метрики удволетворённости

  разобьём пользователей по когортам
  найдём период выборки
  возьмём дату, когда пользователь впервые что-то сделал
first_date = data.groupby(['user_id'])['event_date'].min()
first_date.name = 'start_data'
data = data.join(first_date,on='user_id')
data.head(5)

  найдём когорту

data['active_week'] = pd.to_datetime(data['event_date'],
                                    unit='d') - pd.to_timedelta(data['event_date'].dt.dayofweek, unit='d')
​
​
data['start_week'] = pd.to_datetime(data['start_data'],
                                    unit='d') - pd.to_timedelta(data['start_data'].dt.dayofweek, unit='d')
  найдём лайфтайм

data['cohort_lifetime'] = data['active_week'] - data['start_week']
data['cohort_lifetime'] = data['cohort_lifetime'] / np.timedelta64(1,'W')
data['cohort_lifetime'] = data['cohort_lifetime'].astype(int)
cohorts = data.groupby(['start_week','cohort_lifetime']).agg({'user_id':'nunique'}).reset_index()
 
  найдём число пользователей на начальную неделю

users_count_start = cohorts[cohorts['cohort_lifetime'] == 0][['start_week','user_id']]
​
users_count_start = users_count_start.rename(columns={'user_id':'cohort_users'})
cohorts = cohorts.merge(users_count_start,on='start_week')

  найдём retention rate

cohorts['retention'] = cohorts['user_id']/cohorts['cohort_users']
​
cohorts['start_week'] = cohorts['start_week'].dt.date
​
retention_rate = cohorts.pivot_table(index='start_week',columns='cohort_lifetime',values='retention',aggfunc='sum')
fig = plt.figure(figsize = (10, 5)) 
sns.heatmap(retention_rate, annot = True,fmt = '.2%') 
           
plt.title('коэффицент удержания')  
plt.xlabel('лайфтайм, дни')
plt.ylabel('дата')
plt.show()

    retention rate в первую неделю убывает
    для когорты 07-14 составляет 24.1% , а для пользователей когорты 21-28 всего 21.8% , что явно лучше 
    чем больше людей проводят времени в приложении , тем меньше их остаётся 

3.4  Просмотр контактов
  Просмотр контактов - целевое событие нашего исследования , необходимо изучить его

purpose_event = data.query('event_name == "contacts_show"').groupby('user_id')['event_name'].count().sort_values(ascending=False)

  всего посмотрели 981 раз контакты

purpose_event.describe()

  среднее значение 4.46
  стандартное отклонение 8.97 
  минимальное 1
  максимальное 136
    
    максимальное значение 136 это слишком много , стоит посмотреть выбросы с помощью boxplot

ax = purpose_event.plot.box(figsize=(15, 10))

plt.title('распределение кол-ва просмотра ')

ax.set_ylabel('кол-во просмотров')
ax.set(xticklabels=[])
plt.ylim(0,137);
plt.show()

ax = purpose_event.plot.box(figsize=(15, 10))

plt.title('распределение кол-ва просмотра')

ax.set_ylabel('кол-во просмотров')
ax.set(xticklabels=[])
plt.ylim(0,15);
plt.show()

    136 это единичный случай , скорей всего это ошибка в данных , ну либо человек очень любит покупки
    если ориентироваться на среднее значение 4 , то выбросов много , но они скорей всего абсолютно реальны , люди вполне 
    могут покупать более 4 раз 
    
    более 15 и до примерно 80-60 скорей всего находяться люди , которые занимаются покупками не только для себя , 
    но и для своих товарищей , либо пытаются заработать
    
    на втором графике видно , что значения больше 8 уже являются аномалиями , верхнее значение находиться в 8 , 
    медиана в 2 

3.5  Составим сценарии действий пользователей

data = data.sort_values(['user_id', 'event_time'])
data

    найдём среднее время сессии

    в аналитике https://docs.tracker.my.com/tracking/attribution/attribution_settings
    среднее время таймаута составляет 30 минут , будем использовать его

g = (data.groupby('user_id')['event_time'].diff() > pd.Timedelta('30Min')).cumsum()

data['session_id'] = data.groupby(['user_id', g], sort=False).ngroup() + 1

data = data.sort_values(['user_id','event_time'])
data
    далее избавимся от tips_show , понятие "увидел рекомендованое объявление" не контролируемо и мы не включаем его 
    в сценарий действий пользователя

scen_data = data.query('event_name != "tips_show"')
scen_data['event_name'].unique()
scen_data.head(5)

scen_data_clean = scen_data.groupby('session_id')['event_name'].unique().to_frame()
scen_data_clean = scen_data.groupby('session_id')['event_name'].unique().to_frame()
scen_data_clean


scen_data_clean = scen_data.groupby('session_id')['event_name'].unique().to_frame()
scen_data_clean
sc_start = scen_data_clean
sc_start

sc_start['event_name']



scen_data_clean.event_name = scen_data_clean.event_name.apply(lambda x: tuple(x) if type(x)!= str else tuple([x]))
scen_data_clean.head(10)

a1 = scen_data_clean['event_name'].value_counts()
a1 = a1.to_frame()
a1 = a1.reset_index()
a1

a1 = a1.drop(index=[0,1,2,3,4,5,6,7,8])
a1.head(15)

    Мы видим , что сценарии 9 , 13 , 21 приводят нас к целевому событию , постороим воронку для них.
    возьмём те у которых ЦС стоит на 1 месте

Соответственно нужно построить 3 воронки:

    1.map => contact_show
    2.search => conctact_show
    3.search => photos_show => contacts_show

funnel_data1 = data.loc[data['event_name'].isin(['map', 'contacts_show'])]
ab1 = funnel_data1[funnel_data1['event_name'] == 'map']
ab1 = ab1['user_id'].drop_duplicates()
ab1

#проверка значений
display(ab1.count())
display(data1.query('event_name == "map"')['user_id'].nunique())

map_vor_1 = ab1.count()
ac1 = funnel_data1[(funnel_data1['event_name'] == 'contacts_show') & (funnel_data1['user_id'].isin(ab1))]
ac1 = ac1['user_id'].drop_duplicates()
contacts_vor_1 = ac1.count()

fig = go.Figure(go.Funnel(
    y = ["просмотр карты", "просмотр контактов"],
    x = [map_vor_1,
        contacts_vor_1]))
fig.update_layout(title_text='Воронка просмотр карты => Просмотр контактов')
fig.show()
    
    просматривали карту 1456 человек , из них 289 перешли в контакты

3.5.2  воронка search => contacts_show

funnel_data2 = data.loc[data['event_name'].isin(['search', 'contacts_show'])]
ab2 = funnel_data2[funnel_data2['event_name'] == 'search']
ab2 = ab2['user_id'].drop_duplicates()
ab2

display(ab2.count())
display(data1.query('event_name == "search"')['user_id'].nunique())
search_vor_1 = ab2.count()
ac2 = funnel_data2[(funnel_data2['event_name'] == 'contacts_show') & (funnel_data2['user_id'].isin(ab2))]
ac2 = ac2['user_id'].drop_duplicates()
contacts_vor_2 = ac2.count()
​
fig = go.Figure(go.Funnel(   
    y = ["поиск", "просмотр контактов"],
    x = [search_vor_1,
        contacts_vor_2]))
fig.update_layout(title_text='Воронка Поиск => Просмотр контактов')
fig.show()

    поиском пользовались 1666 человек , контакты из них посмотрели 377

3.5.3  воронка searach => photos_show => contacts_show

funnel_data3 = data.loc[data['event_name'].isin(['search', 'photos_show', 'contacts_show'])]
ab3 = funnel_data3[funnel_data3['event_name'] == 'search']
ab3 = ab3['user_id'].drop_duplicates()
ab3

display(ab3.count())
display(data1.query('event_name == "search"')['user_id'].nunique())

search_vor_3 = ab3.count()
ac3 = funnel_data3[(funnel_data3['event_name'] == 'photos_show') & (funnel_data3['user_id'].isin(ab3))]
ac3 = ac3['user_id'].drop_duplicates()
ac3
photos_vor_3 = ac3.count()
​
ad3 = funnel_data3[(funnel_data3['event_name'] == 'contacts_show') & (funnel_data3['user_id'].isin(ac3))]
ad3 = ad3['user_id'].drop_duplicates()
ad3
contacts_vor_3 = ad3.count()
fig = go.Figure(go.Funnel(
     y = ["поиск", "просмотр фото", "просмотр контактов"],
    x = [search_vor_3,
        photos_vor_3,
        contacts_vor_3]))
fig.update_layout(title_text='Воронка Поиск => Просмотр фото => просмотр контактов')
fig.show()

    поиском пользовались 1666 человек , фото из них посмотрели 647 , а контакты далее посмотрели 192 человека
    в случае с просмотром фото только 12% перешли в ЦС
    В общем показатели близкие , но не очень большие , стоит поработать над ними

3.6  Метрика DAU

DAU - количество уникальных пользователей за сутки (Daily Active Users) 
dau_all = data.groupby('event_date')['user_id'].nunique()
display(dau_all.describe())
dau_all.head(5)
dau_all.plot(kind='hist', bins=5,figsize=(15,10))
​
plt.xlabel('кол-во')
plt.ylabel('частота')
​
​
plt.title('Активные пользователи в день', fontsize=15)
plt.show()
    
    частота активных пользователей примерно одинакова 
    среднее значение -280 
    стандартное отклонение небольшое - 46.737
    минимальное значение 178
    максимальное 352 
    медиана отличается от среднего значения всего на 13 пунктов , что не много

3.7  Относительная частота событий

#создадим 2 группы: 1- совершивших ЦС , 2 - не совершавших ЦС

contact_group = data1.query('event_name == "contacts_show"')['user_id'].unique()
non_contact_group = data1.query('user_id != "contact_group"')['user_id'].unique()

# копия основного датафрейма
data2.info()
group_1 = data2.query('user_id not in @contact_group')['event_name']
group_2 = data2.query('user_id in @contact_group')['event_name']
frame_1 = group_1.value_counts().to_frame()
frame_2 = group_2.value_counts().to_frame()
frame_1 = frame_1.reset_index()
frame_1
frame_2 = frame_2.reset_index()
frame_2

#удалим строчку с contacts_show
frame_2 = frame_2.drop(1)
frame_1_sum = frame_1['event_name'].sum()
frame_2_sum = frame_2['event_name'].sum()
    TE = Target Event(Целевое Событие)

frame_1['TE_ratio%'] = frame_1.apply(lambda row: round((row.event_name / frame_1_sum)*100,2), axis=1)
frame_1 = frame_1.rename(columns={"event_name":"TE_events"})

frame_1
frame_1['TE_ratio%'] = frame_1.apply(lambda row: round((row.event_name / frame_1_sum)*100,2), axis=1)
frame_1 = frame_1.rename(columns={"event_name":"TE_events"})
​
frame_1
frame_2['non_TE_ratio%'] = frame_2.apply(lambda row: round((row.event_name / frame_2_sum)*100,2), axis=1)
frame_2 = frame_2.rename(columns={"event_name":"non_TE_events"})
​
frame_2
both_frames = frame_2.merge(frame_1, on=['index'], how='left')
both_frames
both_frames = both_frames.fillna(0)
both_frames

    показ рекламы ожидаемо находиться на 1 месте в обоих группах (57.24% , 58.52%) и составляет более половины событий
    показ фото больше у пользователей без ЦС (15.69% против 12.63%)
    поиск у пользователей с ЦС немного больше (10.08% против 9.38%)
    открытие карточек у пользоватейлей с ЦС больше ( 9.81% против 7.14%) 
    звонки есть только у пользователей без ЦС - их доля от общего всего 2.41
    люди с ЦС чаще добавляют в избранное (2.13% против 1.90%)
    клики по рекламным объявлениям самые низкие в обеих группах(1.50% пользователи без ЦС и 1.03% у пользователей с ЦС) 

4  Проверка гипотез

4.1  гипотеза 1 *( Одни пользователи совершают действия tips_show и tips_click , другие —
                  только tips_show . Проверьте гипотезу: конверсия в просмотры контактов
                  различается у этих двух групп.)

Проверим гипотезу

Одни пользователи совершают действия tips_show и tips_click , другие —
только tips_show . Проверьте гипотезу: конверсия в просмотры контактов
различается у этих двух групп.
H0: конверсия у двух групп не отличается (группы:

                                                1.tips_show + tips_click
                                                2.tips_show)
H1: конверсии у двух групп отличаются

    выделим две категории: 1.tips_show
                           2.tips_show+click

tips_show = data.query('event_name =="tips_show"')['user_id'].unique().tolist()
print('tips_show count', len(tips_show))
​
tips_show_click = data.query('event_name =="tips_click" and user_id==@tips_show')['user_id'].unique().tolist()
print('tips_show+tips_click count', len(tips_show_click))
​
only_tips = list(set(tips_show) - set(tips_show_click))
print('only_tips ', len(only_tips))
data['event_name'].unique()

data['user_id'].nunique()

    4293 - all 
​    2801 - tips_show
​    297 - tips_show + tips_click

list_10 = ['4293', '2801', '297']
all_users = 4293
tips_show_only = 2801
tips_show_and_tips_click = 297
​
df = pd.DataFrame(list_10)
​
df1 = df
df1 = df1.reset_index()
df3 = {'users : [4293]', 'users2 : [4293]','users3 : [42923]'}
row_labels = ['all_users', 'tips_only', 'tips_and_click']

show_click_events = data.query('user_id==@tips_show_click')[['user_id', 'event_name']]
print('кол-во tips_show + click', show_click_events[show_click_events['event_name']=="contacts_show"]['user_id'].nunique())

only_show_events = data.query('user_id==@only_tips')[['user_id', 'event_name']]
print('кол-во tips_show', only_show_events[only_show_events['event_name']=="contacts_show"]['user_id'].nunique())


    отфильтруем и заполним events
    tips_show count 2504
    tips_show+tips_click count 297
    tips_show_into_cs + click 91 
    tips_show+tips_click__into_cs 425

tips_cs_show_perc = round((91 / 297) * 100, 2)
tips_cs_show_perc

tips_show_click_cs_show_perc = round((425/2504) * 100, 2)
tips_show_click_cs_show_perc

user_group_1 = only_show_events[only_show_events['event_name']=="contacts_show"]['user_id'].nunique()
user_group_2 = show_click_events[show_click_events['event_name']=="contacts_show"]['user_id'].nunique()

events_group_1 = len(only_tips)
events_group_2 = len(tips_show_click)

    будем проверять гипотезу о равенстве долей двух генеральных совокупностей по выборкам из них 

def stat_test(success, not_success, alpha):  
    alpha = alpha
    success = success
    not_success = not_success
    p1 = success[0]/not_success[0]
    p2 = success[1]/not_success[1]
    
    p_combined = (success[0] + success[1]) / (not_success[0] + not_success[1])

    difference = p1 - p2

    z_value = difference / mth.sqrt(p_combined * (1 - p_combined) * (1/not_success[0] + 1/not_success[1]))
    distr = st.norm(0, 1) 
    z_value = difference / mth.sqrt(
        p_combined * (1 - p_combined) * (1 / not_success[0] + 1 / not_success[1]))

    distr = st.norm(0, 1)
    p_value = (1 - distr.cdf(abs(z_value))) * 2

    print('p-значение: ', p_value)

    if p_value < alpha:
        print('Отвергаем нулевую гипотезу: между долями есть значимая разница')
    else:
        print(
            'Не получилось отвергнуть нулевую гипотезу: между долями есть значимая разница'
        ) 


    Возьмём alpha = .05 - критический уровень статистической значимости

alpha = .05
success = [user_group_1, user_group_2]
not_success = [events_group_1, events_group_2]
stat_test(success, not_success, alpha) 

print(success, not_success, alpha)

    Нулевая гипотеза не подтвердилась , между группами есть разница , скорей всего tips_click ведёт к увеличению конверсии

4.2  гипотеза 2

    будем проверять гипотезу о равенстве долей двух генеральных совокупностей по выборкам из них

    проверим гипотезу:у yandex конверсия больше чем у google 

H0: у yandex конверсия меньше или равна google
H1: у yandex конверсия больше google

    подготовим данные

author = ['all_goole_users', 'success_google_users']
article = [1129, 275]
auth_series = pd.Series(author)
article_series = pd.Series(article)
frame = {'event_users': auth_series,
         'count': article_series}
result2 = pd.DataFrame(frame)
result2

plt.figure(figsize=(10,5))
ax=sns.barplot(x=result2['event_users'],y=result2['count'],data = result2, color='steelblue')

    
#добавим числа для понимания
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.1f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha = 'center', va = 'center',
                   xytext = (0, 9),
                   textcoords = 'offset points')
ax.set_xlabel('типы пользователей')
ax.set_ylabel('Число пользователей')
plt.title('Группы пользователей источник google')
plt.show()

google_df = data.query('source == "google"')
yandex_df = data.query('source == "yandex"')

yandex_users = yandex_df.user_id.nunique()
google_users = google_df.user_id.nunique()

success_yandex = yandex_df.query('event_name=="contacts_show" & source == "yandex"').user_id.nunique()
success_google = google_df.query('event_name=="contacts_show"').user_id.nunique()

conversion_yandex = success_yandex / yandex_users
conversion_google = success_google / google_users

success_yandex
yandex_users
    
    Возьмём alpha = .05 - критический уровень статистической значимости

aplha = .05


success = np.array([success_google, success_yandex])
not_success = np.array([google_users , yandex_users]) 

p1 = success[0]/not_success[0]
p2 = success[1]/not_success[1]

p_combined = (success[0] + success[1]) / (not_success[0] + not_success[1])
difference = p1 - p2 
z_value = difference / mth.sqrt(p_combined * (1 - p_combined) * (1/not_success[0] + 1/not_success[1]))
distr = st.norm(0, 1)  

z_value = difference / mth.sqrt(p_combined * (1 - p_combined) * (1/not_success[0] + 1/not_success[1]))
distr = st.norm(0, 1) 

p_value = (1 - distr.cdf(abs(z_value))) * 2
print(f'p_values = {p_value}')

if (p_value < alpha):
    print("Отвергаем нулевую гипотезу:у yandex конверсия меньше или равна google")
else:
    print("Не получилось отвергнуть нулевую гипотезу,у yandex конверсия меньше или равна google") 
print(f'конверсия yandex: {conversion_yandex:.2%}')
print(f'конверсия  google: {conversion_google:.2%}')  

    Не получилось отвергнуть гипотезу "у yandex конверсия меньше или равна google"

4.3  Выводы

Следующие шаги были реализованы:

-Загрузили данные , провели общий осмотр.
-Изменили тип данных в одной колонке , убрали милисекунды, поменяли названия колонок.
-Проверили наличие дубликатов и пропусков , убрали дубликаты.
-Совместили два датафрейма в один и удостоверились что нет пропусков и дубликатов в ней нет
-Посчитали долю из каждого источника установки , посмотрели график(1-yandex, 2-other,3-google)
-Посчитали частоту действий пользователей , посмотрели график (1-tips_show,2-photo_show,3-search)
-Рассмотрели коэффицент удержания клиентов,начале примерно 21% , так же выяснили что чем дольше пользователть проводит 
    времени в приложении , тем меньше люди остаются.
-Рассмотрели DAU(Daily Active Users) , частота примерно одинаковая , в среднем 280 человек в день активны , 
    в особые периоды максимальное значение доходит до 352
-Составили сценарии действий пользователей и выяснили , что в ЦС больше всего переходов после map(просмотр карты) - 147 , 
    потом search(поиск)-99 и photo_show(просмотр фото)-77
-изучили относительную частоту событий и выяснили что фото(15.69% > 12.63%) больше просматривают пользователи без ЦС , 
    а вот поиск(10.08% > 9.38%) и открытие карточек(9.81% > 7.14%) больше у людей с ЦС. звонки идут только от людей без ЦС
-Рассмотрели гипотезу "Одни пользователи совершают действия tips_show и tips_click , другие —
    только tips_show . Проверили гипотезу: конверсия в просмотры контактов
    различается у этих двух групп." , поняли , что между этими группами есть разница.
-Рассмотрели гипотезу "Пользователи которые пришли с источника Yandex (source) имеют лучшую конверсию в просмотры контактов ,
    чем пользователи пришедшие с Google." 
    Выяснили , что у двух площадок очень близкая конверсия в просмотры (24.72% - yandex , 24.36% - google).


    -показ рекламы ожидаемо находиться на 1 месте в обоих группах (57.24% , 58.52%) и составляет более половины событий
    -показ фото больше у пользователей без ЦС (15.69% против 12.63%)
    -поиск у пользователей с ЦС немного больше (10.08% против 9.38%)
    -открытие карточек у пользоватейлей с ЦС больше ( 9.81% против 7.14%) 
    -звонки есть только у пользователей без ЦС - их доля от общего всего 2.41
    -люди с ЦС чаще добавляют в избранное (2.13% против 1.90%)
    -клики по рекламным объявлениям самые низкие в обеих группах(1.50% пользователи без ЦС и 1.03% у пользователей с ЦС) 


Общие рекомендации:

-yandex является самым успешным источником установки приложений , а у google позиции хуже , 
    хотя ползьватели показывают похожую конверсию , можно увеличить бюджет на привлечение через google
-просмотр контактов очень важен , его увеличение ведёт к росту показателей: поиск , 
    открытие карточек и добавления в избранное. стоит больше предлагать людям разнообразных товаров , рости "вширь" и 
    брать кол-вом 
-подумать над способами удержания старых пользователей-например,разработать премиум - подписку, 
    которое будет повышать в рейтинге размещенные пользователями объявления
-работать над метриками , улучшающими ЦС - map , search , photo_show лучше всего улучшают ЦС
