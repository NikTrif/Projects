Проект по теме "Анализ поведения пользователей магазина << Ненужные вещи>>"

Описание проекта:

    Наши пользователи совершают много действий в приложении, и мы уверены, что в этих
    данных есть инсайты, которые позволят нам стать лучшим приложением для продажи 
    ненужных вещей.
    «Ненужные вещи» — ваши ненужные вещи нужны кому-то другому!
Задача:

  1 Проанализируйте связь целевого события — просмотра контактов — и других
  действий пользователей.

  2 Оцените, какие действия чаще совершают те пользователи, которые
  просматривают контакты.
  Проведите исследовательский анализ данных
  Проанализируйте влияние событий на совершение целевого события
  Этапы:


  Проведите исследовательский анализ данных

  Проанализируйте влияние событий на совершение целевого события

  Проверить гипотезы:
    1 Одни пользователи совершают действия tips_show и tips_click , другие —
    только tips_show . Проверьте гипотезу: конверсия в просмотры контактов
    различается у этих двух групп.

    2 Сформулируйте собственную статистическую гипотезу. Дополните её
    нулевой и альтернативной гипотезами. Проверьте гипотезу с помощью
    статистического теста.



import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
import numpy as np
import math as mth
import scipy.stats as stats
from scipy import stats as st
import plotly.express as px
from plotly import graph_objects as go
import warnings
warnings.filterwarnings('ignore')


1 Загрузка данных (загрузка необходимых данных , первичный осмотр)

data_df = pd.read_csv('https://code.s3.yandex.net/datasets/mobile_dataset.csv')
source_df = pd.read_csv('https://code.s3.yandex.net/datasets/mobile_sources.csv')
​
display(data_df.head(5))
display(source_df.head(5))

display(data_df.info())
display(source_df.info())

      Два датафрейма:
      первый состоит из 3 колонок , 74197 строк. 
      второй из 2 колонок , 4293 строк.
      Видим что в большинстве типы данных приемлимые(object) , кроме колонки event.time , нужно  поменять на datetime, 
      так же стоит убрать милисекунды.
      Нужно заменить в названиях точки (event.time) на нижние пробелы (event_time) для всех столбцов data_df.
      В source_df поменять название колонки userId на user_id.


2  Предобработка данных
  Поменяем тип данных и названия

  уберём милисекунды из event.time

data_df['event.time'] = pd.to_datetime(data_df['event.time'])
data_df['event.time'] = data_df['event.time'].dt.floor('s')
data_df.info()

data_df = data_df.rename(columns={"event.name" : "event_name", "event.time" : "event_time" , "user.id" : "user_id"})
source_df = source_df.rename(columns={"userId" : "user_id"})
display(data_df.head(5))
display(source_df.head(5))

  Все что нужно поменяли , теперь перейдём к дубликатам и пропускам

print('Количество дубликатов в таблице data_df:', data_df.duplicated().sum())

  Удалим дубликаты , их не очень много , почти не повлияет на общую выборку

data_df = data_df.drop_duplicates().reset_index(drop=True)

print(data_df.isna().sum())
print(source_df.isna().sum())
  
  пропусков не наблюдается
  ещё стоит посмотреть уникальные значения колонок с названиями действий пользователей и источника установки приложения

print(data_df['event_name'].unique())
print(source_df['source'].unique())

print('Количество дубликатов в общей таблице data:', data.duplicated().sum())
print('Количество пропущенных значений в общей таблице data:', data.isnull().sum())

  Предобработали данные и создали один общий датафрейм data для работы с ним
  добавим туда колонки с часом , неделей , месяцем , годом и просто датой без времени

data.head(5)

  Создадим резервные датафреймы

data1=data
data2=data
data3=data

display(data.query('event_name == "map"')['user_id'].nunique())
display(data2.query('event_name == "map"')['user_id'].nunique())
display(data3.query('event_name == "map"')['user_id'].nunique())

3  Исследовательский анализ данных
  Рассмотрим данные и немного исследуем их

3.1  Посчитаем долю из каждого источника установки
global_sc = data.groupby('source').agg({'user_id' : 'nunique'})

global_sc = global_sc.reset_index()
global_sc

global_sc['proportion'] = global_sc['user_id'] / len(data['user_id'].unique()) *100
global_sc

global_sc = global_sc.round(1)
global_sc

plt.figure(figsize=(10,5))
ax=sns.barplot(x=global_sc['source'],y=global_sc['proportion'], data = global_sc, color='steelblue')
​
#добавим числа для понимания
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.1f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha = 'center', va = 'center',
                   xytext = (0, 9),
                   textcoords = 'offset points')
​
​
ax.set_xlabel('Источник установки приложения')
ax.set_ylabel('Процент пользователей')
plt.title('Источники установки приложения')
​
plt.show()
    
    yandex на 1 месте среди всех источников скачивания приложений (1934 - 45.1%)
    other(другие) занимает второе место (1230 - 28.7%)
    далее идёт google (1129 - 26.3%)

3.2  посмотрим частоту действий пользователей
    
    для начала нужно привести в порядок колонку event_name
    нужно совместить contacts_show и show_contacts. Так же поступим с всеми поисковыми search_N(1-7)


def event_combination(i):
    
    if i == 'show_contacts':
        
        new_name = 'contacts_show'
        return new_name
    
    elif 'search' in i:
        
        new_name = 'search'
        return new_name
    
    return i
data['event_name'] = data['event_name'].apply(event_combination)
freq_events = data['event_name'].value_counts().to_frame()
freq_events

data['event_name'] = data['event_name'].apply(event_combination)
freq_events = data['event_name'].value_counts().to_frame()
freq_events

freq_events.plot(kind='bar', figsize=(10,5))

plt.title('Частота совершения действий', fontsize=15)

plt.gca().set(xlabel='типы событий', ylabel='количество событий')
plt.xticks(rotation=20)

plt.show()

    на первом месте находится tips_show - увидел рекомендованные объявлений , почти 40000 , оно учитывается автоматически ,
    не зависит от действий пользователя
    далее идуёт photos_show - просмотр фото в объявлении
    на 3 месте поиск по сайту
    4 место - advert_open - открыл карточки объявлений
    5 - map - открытие карты объявлений
    favorites_add , tips_click , contacts_call занимают 3 последних места - избранное , клик по рекомендации и звонок
    tips_show - сильно больше любых других действий, вызвано это тем что люди постоянно смотрят рекомендованные объявления в приложении с продажей для покупки

3.3  Retention rate
  посмотрим метрики удволетворённости

  разобьём пользователей по когортам
  найдём период выборки
  возьмём дату, когда пользователь впервые что-то сделал
first_date = data.groupby(['user_id'])['event_date'].min()
first_date.name = 'start_data'
data = data.join(first_date,on='user_id')
data.head(5)

  найдём когорту

data['active_week'] = pd.to_datetime(data['event_date'],
                                    unit='d') - pd.to_timedelta(data['event_date'].dt.dayofweek, unit='d')
​
​
data['start_week'] = pd.to_datetime(data['start_data'],
                                    unit='d') - pd.to_timedelta(data['start_data'].dt.dayofweek, unit='d')
  найдём лайфтайм

data['cohort_lifetime'] = data['active_week'] - data['start_week']
data['cohort_lifetime'] = data['cohort_lifetime'] / np.timedelta64(1,'W')
data['cohort_lifetime'] = data['cohort_lifetime'].astype(int)
cohorts = data.groupby(['start_week','cohort_lifetime']).agg({'user_id':'nunique'}).reset_index()
 
  найдём число пользователей на начальную неделю

users_count_start = cohorts[cohorts['cohort_lifetime'] == 0][['start_week','user_id']]
​
users_count_start = users_count_start.rename(columns={'user_id':'cohort_users'})
cohorts = cohorts.merge(users_count_start,on='start_week')

  найдём retention rate

cohorts['retention'] = cohorts['user_id']/cohorts['cohort_users']
​
cohorts['start_week'] = cohorts['start_week'].dt.date
​
retention_rate = cohorts.pivot_table(index='start_week',columns='cohort_lifetime',values='retention',aggfunc='sum')
fig = plt.figure(figsize = (10, 5)) 
sns.heatmap(retention_rate, annot = True,fmt = '.2%') 
           
plt.title('коэффицент удержания')  
plt.xlabel('лайфтайм, дни')
plt.ylabel('дата')
plt.show()

    retention rate в первую неделю убывает
    для когорты 07-14 составляет 24.1% , а для пользователей когорты 21-28 всего 21.8% , что явно лучше 
    чем больше людей проводят времени в приложении , тем меньше их остаётся 

3.4  Просмотр контактов
  Просмотр контактов - целевое событие нашего исследования , необходимо изучить его

purpose_event = data.query('event_name == "contacts_show"').groupby('user_id')['event_name'].count().sort_values(ascending=False)

  всего посмотрели 981 раз контакты

purpose_event.describe()

  среднее значение 4.46
  стандартное отклонение 8.97 
  минимальное 1
  максимальное 136
    
    максимальное значение 136 это слишком много , стоит посмотреть выбросы с помощью boxplot

ax = purpose_event.plot.box(figsize=(15, 10))

plt.title('распределение кол-ва просмотра ')

ax.set_ylabel('кол-во просмотров')
ax.set(xticklabels=[])
plt.ylim(0,137);
plt.show()

ax = purpose_event.plot.box(figsize=(15, 10))

plt.title('распределение кол-ва просмотра')

ax.set_ylabel('кол-во просмотров')
ax.set(xticklabels=[])
plt.ylim(0,15);
plt.show()

    136 это единичный случай , скорей всего это ошибка в данных , ну либо человек очень любит покупки
    если ориентироваться на среднее значение 4 , то выбросов много , но они скорей всего абсолютно реальны , люди вполне 
    могут покупать более 4 раз 
    
    более 15 и до примерно 80-60 скорей всего находяться люди , которые занимаются покупками не только для себя , 
    но и для своих товарищей , либо пытаются заработать
    
    на втором графике видно , что значения больше 8 уже являются аномалиями , верхнее значение находиться в 8 , 
    медиана в 2 

3.5  Составим сценарии действий пользователей

data = data.sort_values(['user_id', 'event_time'])
data

    найдём среднее время сессии

    в аналитике https://docs.tracker.my.com/tracking/attribution/attribution_settings
    среднее время таймаута составляет 30 минут , будем использовать его

g = (data.groupby('user_id')['event_time'].diff() > pd.Timedelta('30Min')).cumsum()

data['session_id'] = data.groupby(['user_id', g], sort=False).ngroup() + 1

data = data.sort_values(['user_id','event_time'])
data
    далее избавимся от tips_show , понятие "увидел рекомендованое объявление" не контролируемо и мы не включаем его 
    в сценарий действий пользователя

scen_data = data.query('event_name != "tips_show"')
scen_data['event_name'].unique()
scen_data.head(5)

scen_data_clean = scen_data.groupby('session_id')['event_name'].unique().to_frame()
scen_data_clean = scen_data.groupby('session_id')['event_name'].unique().to_frame()
scen_data_clean


scen_data_clean = scen_data.groupby('session_id')['event_name'].unique().to_frame()
scen_data_clean
sc_start = scen_data_clean
sc_start

sc_start['event_name']



scen_data_clean.event_name = scen_data_clean.event_name.apply(lambda x: tuple(x) if type(x)!= str else tuple([x]))
scen_data_clean.head(10)

a1 = scen_data_clean['event_name'].value_counts()
a1 = a1.to_frame()
a1 = a1.reset_index()
a1

a1 = a1.drop(index=[0,1,2,3,4,5,6,7,8])
a1.head(15)

    Мы видим , что сценарии 9 , 13 , 21 приводят нас к целевому событию , постороим воронку для них.
    возьмём те у которых ЦС стоит на 1 месте

Соответственно нужно построить 3 воронки:

    1.map => contact_show
    2.search => conctact_show
    3.search => photos_show => contacts_show

funnel_data1 = data.loc[data['event_name'].isin(['map', 'contacts_show'])]
ab1 = funnel_data1[funnel_data1['event_name'] == 'map']
ab1 = ab1['user_id'].drop_duplicates()
ab1

#проверка значений
display(ab1.count())
display(data1.query('event_name == "map"')['user_id'].nunique())

map_vor_1 = ab1.count()
ac1 = funnel_data1[(funnel_data1['event_name'] == 'contacts_show') & (funnel_data1['user_id'].isin(ab1))]
ac1 = ac1['user_id'].drop_duplicates()
contacts_vor_1 = ac1.count()

fig = go.Figure(go.Funnel(
    y = ["просмотр карты", "просмотр контактов"],
    x = [map_vor_1,
        contacts_vor_1]))
fig.update_layout(title_text='Воронка просмотр карты => Просмотр контактов')
fig.show()
    
    просматривали карту 1456 человек , из них 289 перешли в контакты

3.5.2  воронка search => contacts_show

funnel_data2 = data.loc[data['event_name'].isin(['search', 'contacts_show'])]
ab2 = funnel_data2[funnel_data2['event_name'] == 'search']
ab2 = ab2['user_id'].drop_duplicates()
ab2

display(ab2.count())
display(data1.query('event_name == "search"')['user_id'].nunique())
search_vor_1 = ab2.count()
ac2 = funnel_data2[(funnel_data2['event_name'] == 'contacts_show') & (funnel_data2['user_id'].isin(ab2))]
ac2 = ac2['user_id'].drop_duplicates()
contacts_vor_2 = ac2.count()
​
fig = go.Figure(go.Funnel(   
    y = ["поиск", "просмотр контактов"],
    x = [search_vor_1,
        contacts_vor_2]))
fig.update_layout(title_text='Воронка Поиск => Просмотр контактов')
fig.show()

    поиском пользовались 1666 человек , контакты из них посмотрели 377

3.5.3  воронка searach => photos_show => contacts_show

funnel_data3 = data.loc[data['event_name'].isin(['search', 'photos_show', 'contacts_show'])]
ab3 = funnel_data3[funnel_data3['event_name'] == 'search']
ab3 = ab3['user_id'].drop_duplicates()
ab3

display(ab3.count())
display(data1.query('event_name == "search"')['user_id'].nunique())

search_vor_3 = ab3.count()
ac3 = funnel_data3[(funnel_data3['event_name'] == 'photos_show') & (funnel_data3['user_id'].isin(ab3))]
ac3 = ac3['user_id'].drop_duplicates()
ac3
photos_vor_3 = ac3.count()
​
ad3 = funnel_data3[(funnel_data3['event_name'] == 'contacts_show') & (funnel_data3['user_id'].isin(ac3))]
ad3 = ad3['user_id'].drop_duplicates()
ad3
contacts_vor_3 = ad3.count()
fig = go.Figure(go.Funnel(
     y = ["поиск", "просмотр фото", "просмотр контактов"],
    x = [search_vor_3,
        photos_vor_3,
        contacts_vor_3]))
fig.update_layout(title_text='Воронка Поиск => Просмотр фото => просмотр контактов')
fig.show()

    поиском пользовались 1666 человек , фото из них посмотрели 647 , а контакты далее посмотрели 192 человека
    в случае с просмотром фото только 12% перешли в ЦС
    В общем показатели близкие , но не очень большие , стоит поработать над ними

3.6  Метрика DAU

DAU - количество уникальных пользователей за сутки (Daily Active Users) 
dau_all = data.groupby('event_date')['user_id'].nunique()
display(dau_all.describe())
dau_all.head(5)
dau_all.plot(kind='hist', bins=5,figsize=(15,10))
​
plt.xlabel('кол-во')
plt.ylabel('частота')
​
​
plt.title('Активные пользователи в день', fontsize=15)
plt.show()
    частота активных пользователей примерно одинакова 
    среднее значение -280 
    стандартное отклонение небольшое - 46.737
    минимальное значение 178
    максимальное 352 
    медиана отличается от среднего значения всего на 13 пунктов , что не много

3.7  Относительная частота событий

#создадим 2 группы: 1- совершивших ЦС , 2 - не совершавших ЦС

contact_group = data1.query('event_name == "contacts_show"')['user_id'].unique()
non_contact_group = data1.query('user_id != "contact_group"')['user_id'].unique()

# копия основного датафрейма
data2.info()
group_1 = data2.query('user_id not in @contact_group')['event_name']
group_2 = data2.query('user_id in @contact_group')['event_name']
frame_1 = group_1.value_counts().to_frame()
frame_2 = group_2.value_counts().to_frame()
frame_1 = frame_1.reset_index()
frame_1
frame_2 = frame_2.reset_index()
frame_2

#удалим строчку с contacts_show
frame_2 = frame_2.drop(1)
frame_1_sum = frame_1['event_name'].sum()
frame_2_sum = frame_2['event_name'].sum()
    TE = Target Event(Целевое Событие)

frame_1['TE_ratio%'] = frame_1.apply(lambda row: round((row.event_name / frame_1_sum)*100,2), axis=1)
frame_1 = frame_1.rename(columns={"event_name":"TE_events"})

frame_1
frame_1['TE_ratio%'] = frame_1.apply(lambda row: round((row.event_name / frame_1_sum)*100,2), axis=1)
frame_1 = frame_1.rename(columns={"event_name":"TE_events"})
​
frame_1
frame_2['non_TE_ratio%'] = frame_2.apply(lambda row: round((row.event_name / frame_2_sum)*100,2), axis=1)
frame_2 = frame_2.rename(columns={"event_name":"non_TE_events"})
​
frame_2
both_frames = frame_2.merge(frame_1, on=['index'], how='left')
both_frames
both_frames = both_frames.fillna(0)
both_frames

    показ рекламы ожидаемо находиться на 1 месте в обоих группах (57.24% , 58.52%) и составляет более половины событий
    показ фото больше у пользователей без ЦС (15.69% против 12.63%)
    поиск у пользователей с ЦС немного больше (10.08% против 9.38%)
    открытие карточек у пользоватейлей с ЦС больше ( 9.81% против 7.14%) 
    звонки есть только у пользователей без ЦС - их доля от общего всего 2.41
    люди с ЦС чаще добавляют в избранное (2.13% против 1.90%)
    клики по рекламным объявлениям самые низкие в обеих группах(1.50% пользователи без ЦС и 1.03% у пользователей с ЦС) 

4  Проверка гипотез

4.1  гипотеза 1

Проверим гипотезу

Одни пользователи совершают действия tips_show и tips_click , другие —
только tips_show . Проверьте гипотезу: конверсия в просмотры контактов
различается у этих двух групп.
H0: конверсия у двух групп не отличается (группы:

                                                1.tips_show + tips_click
                                                2.tips_show)
H1: конверсии у двух групп отличаются

    выделим две категории: 1.tips_show
                           2.tips_show+click

tips_show = data.query('event_name =="tips_show"')['user_id'].unique().tolist()
print('tips_show count', len(tips_show))
​
tips_show_click = data.query('event_name =="tips_click" and user_id==@tips_show')['user_id'].unique().tolist()
print('tips_show+tips_click count', len(tips_show_click))
​
only_tips = list(set(tips_show) - set(tips_show_click))
print('only_tips ', len(only_tips))
data['event_name'].unique()
